
Этот итерационный метод имеет некоторое сходство с [[Алгоритм Хука и Дживса]] Метод Розенброка также называется методом вращающихся координат. Этот метод существенно **эффективнее предыдущих методов, особенно при минимизации функций овражного типа.**

Общая идея метода заключается в том, что выбирается система ортогональных направлений $\bar{S}_1^0, \bar{S}_2^0, \dots, \bar{S}_n^0$, в каждом из которых последовательно ищется минимальное значение, после чего **система направлений поворачивается** так, чтобы одна из осей совпала с направлением полного перемещения, а остальные были ортогональны между собой.

Пусть $\bar{x}^0$ – вектор начального приближения; $\bar{S}_1^0, \bar{S}_2^0, \dots, \bar{S}_n^0$ – система ортогональных направлений. На первой итерации это может быть ортонормированная система координат.

Начиная с $\bar{x}^0$, последовательно осуществляем минимизацию функции $f(\bar{x})$ в соответствующих направлениях $\bar{S}_1^0, \bar{S}_2^0, \dots, \bar{S}_n^0$, находя последовательные приближения:
$$\bar{x}_1^0 = \bar{x}^0 + \lambda_1 \bar{S}_1^0, \quad \lambda_1 = \arg \min_{\lambda} f(\bar{x}^0 + \lambda \bar{S}_1^0),$$
...
$$\bar{x}_n^0 = \bar{x}_{n-1}^0 + \lambda_n \bar{S}_n^0, \quad \lambda_n = \arg \min_{\lambda} f(\bar{x}_{n-1}^0 + \lambda \bar{S}_n^0).$$

Следующая итерация начнется с точки $\bar{x}^1 = \bar{x}_n^0$. Если не изменить систему направлений, то будем иметь [[Алгоритм Гаусса (Покоординатный спуск)]]. Поэтому после завершения очередного $k$-го этапа **вычисляем новые направления поиска**.

---

## Поворот ортогональных направлений

Ортогональные направления поиска поворачиваются так, чтобы они оказались вытянутыми вдоль "оврага" ("хребта") и, таким образом, исключается взаимодействие переменных ($x_i, x_j$). Направления поиска вытягиваются вдоль главных осей квадратичной аппроксимации целевой функции.

Рассмотрим некоторую $k$-ю итерацию алгоритма Розенброка.
В результате минимизации по каждому из ортогональных направлений на данной итерации мы имеем систему параметров $\lambda_1^k, \lambda_2^k, \dots, \lambda_n^k$, с помощью которых определим систему векторов $\bar{A}_1^k, \bar{A}_2^k, \dots, \bar{A}_n^k$, вычисляемых по формулам следующего вида:
$$\bar{A}_1^k = \lambda_1^k \bar{S}_1^k + \lambda_2^k \bar{S}_2^k + \dots + \lambda_n^k \bar{S}_n^k;$$
$$\bar{A}_2^k = \lambda_2^k \bar{S}_2^k + \dots + \lambda_n^k \bar{S}_n^k;$$
...
$$\bar{A}_n^k = \lambda_n^k \bar{S}_n^k.$$

С помощью системы векторов $\bar{A}_1^k, \bar{A}_2^k, \dots, \bar{A}_n^k$ строим новую систему ортогональных направлений $\bar{S}_1^{k+1}, \bar{S}_2^{k+1}, \dots, \bar{S}_n^{k+1}$.

Причем первый вектор направляют так, чтобы он совпал с направлением общего перемещения на $k$-м шаге, а остальные получаются с помощью процедуры **ортогонализации Грама-Шмидта**:
$$\bar{S}_1^{k+1} = \frac{\bar{A}_1^k}{||\bar{A}_1^k||};$$
$$\bar{B}_l^k = \bar{A}_l^k - \sum_{m=1}^{l-1} (\bar{A}_l^k)^T \bar{S}_m^{k+1} \bar{S}_m^{k+1};$$
$$\bar{S}_l^{k+1} = \frac{\bar{B}_l^k}{||\bar{B}_l^k||}, \quad l=2,\dots,n.$$

---

## Критерии останова и коэффициенты

Палмером было показано, что векторы $\bar{B}_j^{k+1}$ и $||\bar{B}_j^{k+1}||$ пропорциональны $\lambda_j^k$ (при условии, что $\sum_{i=j}^n (\lambda_i^k)^2 \ne 0$).

> [!INFO] Следовательно, при вычислении $\bar{S}_j^{k+1} = \bar{B}_j^k / ||\bar{B}_j^k||$, величина $\lambda_j^k$ сокращается, и, таким образом, вектор $\bar{S}_j^{k+1}$ **остается определенным**, даже если $\lambda_j^k = 0$.

Имея это в виду, Палмер предложил для вычисления $\bar{S}_j^{k+1}$ использовать следующие соотношения:
$$\bar{A}_i^k = \sum_{j=i}^n \lambda_j^k \bar{S}_j^k, \quad i = 1,\dots,n,$$
$$\bar{S}_j^{k+1} = \frac{\bar{A}_{i-1}^k \bar{A}_{i-1}^k - \bar{A}_i^k \bar{A}_i^k}{\sqrt{||\bar{A}_{i-1}^k||^2 - ||\bar{A}_i^k||^2}}, \quad i = 2,\dots,n,$$
$$\bar{S}_1^{k+1} = \frac{\bar{A}_1^k}{||\bar{A}_1^k||}.$$

Критерии останова алгоритма могут быть стандартными, как и описанные в предыдущих алгоритмах прямых методов.