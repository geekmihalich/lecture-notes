Это простейший алгоритм, заключающийся в том, что на каждом шаге (каждой итерации) минимизация осуществляется только по одной компоненте вектора переменных $\bar{x}$.

Пусть нам дано начальное приближение $\bar{x}^0 = (x_1^0, x_2^0, \dots, x_n^0)^T$.

На первой итерации находим значение минимума функции при изменяющейся первой координате и фиксированных остальных компонентах, т.е.:
$$\bar{x}_1^1 = \arg \min_{x_1} f(x_1, x_2^0, \dots, x_n^0).$$

В результате получаем новую точку $\bar{x}^1 = (x_1^1, x_2^0, \dots, x_n^0)$.

Далее из точки $\bar{x}^1$ ищем минимум функции, изменяя вторую координату и считая фиксированными все остальные координаты. В результате получаем значение:
$$\bar{x}_2^2 = \arg \min_{x_2} f(x_1^1, x_2, x_3^0, \dots, x_n^0)$$
и новую точку $\bar{x}^2 = (x_1^1, x_2^2, x_3^0, \dots, x_n^0)$.

Продолжая процесс, после $n$ шагов получаем точку $\bar{x}^n = (x_1^n, x_2^n, \dots, x_n^n)$, начиная с которой процесс возобновляется.

---

## Условия прекращения поиска

В качестве условий прекращения поиска можно использовать следующие два критерия:
1.  $$|f(\bar{x}^{k+1}) - f(\bar{x}^k)| \le \varepsilon_0.$$
2.  $$|x_i^{k+1} - x_i^k| \le \varepsilon_1, \forall i.$$

---

## Особенности алгоритма Гаусса

> [!WARNING] Метод очень прост, но **не очень эффективен**.

Проблемы могут возникнуть, когда линии уровня сильно вытянуты и "эллипсоиды" ориентированы, например, вдоль прямых вида $x_1 = x_2$. В подобной ситуации поиск быстро застревает на дне такого оврага, а если начальное приближение оказывается на оси "эллипсоида", то процесс так и останется в этой точке.
> [!TIP] Хорошие результаты получаются в тех случаях, когда целевая функция представляет собой **выпуклую сепарабельную функцию** вида:
> $$f(\bar{x}) = \sum_{i=1}^n f_i(x_i).$$