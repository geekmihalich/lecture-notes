В [[Алгоритм наискорейшего спуска|алгоритме наискорейшего спуска]] на каждом этапе поиска используется только текущая информация о функции $f(\bar{x}^k)$ и градиенте $\nabla f(\bar{x}^k)$. В алгоритмах сопряженных градиентов используется информация о поиске на предыдущих этапах спуска.

Направление поиска на текущем шаге $\bar{S}^k$ строится как линейная комбинация наискорейшего спуска на данном шаге $-\nabla f(\bar{x}^k)$ и направлений спуска на предыдущих шагах $\bar{S}^0, \bar{S}^1, \dots, \bar{S}^{k-1}$.

> [!INFO] Веса в линейной комбинации выбираются таким образом, чтобы сделать эти направления **сопряженными**. В этом случае квадратичная функция будет минимизироваться за $n$ шагов алгоритма.

> [!TIP] При выборе весов используется только текущий градиент и градиент в предыдущей точке.

---

## Алгоритм (Флетчера-Ривса)

В начальной точке $\bar{x}^0$ направление спуска $\bar{S}^0 = -\nabla f(\bar{x}^0)$:
$$\bar{x}^1 = \bar{x}^0 + \lambda^0 \cdot \bar{S}^0, \quad (1)$$
где $\lambda^0$ выбирается из соображений минимальности целевой функции в данном направлении:
$$\lambda^0 = \arg \min_{\lambda} f(\bar{x}^0 + \lambda \cdot \bar{S}^0).$$

Новое направление поиска:
$$\bar{S}^1 = -\nabla f(\bar{x}^1) + \omega_1 \cdot \bar{S}^0, \quad (2)$$
где $\omega_1$ выбирается так, чтобы сделать направления $\bar{S}^1$ и $\bar{S}^0$ сопряженными по отношению к матрице $H$:
$$(\bar{S}^0)^T \cdot H \cdot \bar{S}^1 = 0. \quad (3)$$

Для квадратичной функции справедливы соотношения:
$f(\bar{x}) = f(\bar{x}^0) + \nabla^T f(\bar{x}^0) \cdot \Delta \bar{x} + \frac{1}{2}(\Delta \bar{x})^T \cdot \nabla^2 f(\bar{x}^0) \cdot \Delta \bar{x},$
где $\Delta \bar{x} = \bar{x} - \bar{x}^0,$
$\nabla f(\bar{x}) = \nabla f(\bar{x}^0) + \nabla^2 f(\bar{x}^0) \cdot \Delta \bar{x}.$
Если положить $\bar{x} = \bar{x}^1$, тогда $\bar{x}^1 - \bar{x}^0 = \lambda^0 \bar{S}^0$ и
$$\nabla f(\bar{x}^1) - \nabla f(\bar{x}^0) = \nabla^2 f(\bar{x}^0) \cdot (\bar{x}^1 - \bar{x}^0) = H (\lambda^0 \bar{S}^0). \quad (4)$$

Воспользуемся (4), чтобы исключить $(\bar{S}^0)^T H$ из уравнения (3). Для квадратичной функции $H = H^T$, поэтому транспонировав (4) и умножив справа на $H^{-1}$, получим:
$$[\nabla f(\bar{x}^1) - \nabla f(\bar{x}^0)]^T H^{-1} = (\bar{x}^1 - \bar{x}^0)^T H H^{-1}$$
и далее (после некоторых выкладок):
$$\bar{S}^0 = \frac{(\bar{x}^1 - \bar{x}^0)^T H^{-1}}{ \lambda^0} = \frac{[\nabla f(\bar{x}^1) - \nabla f(\bar{x}^0)]^T H^{-1}}{\lambda^0}.$$
Следовательно, для сопряженности $\bar{S}^0$ и $\bar{S}^1$ необходимо:
$$[\nabla f(\bar{x}^1) - \nabla f(\bar{x}^0)]^T H^{-1} H \bar{S}^0 + \omega_1 (\bar{S}^0)^T H \bar{S}^0 = 0.$$
> [!INFO] Вследствие изложенных ранее свойств сопряженности все перекрестные члены исчезают.
Учитывая, что $\bar{S}^0 = -\nabla f(\bar{x}^0)$ и, следовательно:
$$(-\nabla f(\bar{x}^0))^T [\nabla f(\bar{x}^1) - \nabla f(\bar{x}^0)] + \omega_1 (-\nabla f(\bar{x}^0))^T (-\nabla f(\bar{x}^0)) = 0$$
получим для $\omega_1$ следующее соотношение:
$$\omega_1 = \frac{\nabla^T f(\bar{x}^1) \cdot \nabla f(\bar{x}^1)}{\nabla^T f(\bar{x}^0) \cdot \nabla f(\bar{x}^0)} = \frac{||\nabla f(\bar{x}^1)||^2}{||\nabla f(\bar{x}^0)||^2}. \quad (5)$$

Направление поиска $\bar{S}^2$ строится в виде линейной комбинации векторов $-\nabla f(\bar{x}^2), \bar{S}^0, \bar{S}^1$, причем так, чтобы оно было сопряженным с $\bar{S}^1$.
Если распространить сделанные выкладки на $\bar{S}^2, \bar{S}^3, \dots$, опуская их содержание и учитывая, что $(\bar{S}^k)^T \nabla f(\bar{x}^{k+1}) = 0$ приводит к $\nabla^T f(\bar{x}^k) \nabla f(\bar{x}^{k+1}) = 0$, можно получить общее выражение для $\omega_k$:
$$\omega_k = \frac{||\nabla f(\bar{x}^k)||^2}{||\nabla f(\bar{x}^{k-1})||^2}. \quad (6)$$
> [!INFO] Все весовые коэффициенты, предшествующие $\omega_k$, что **особенно интересно**, оказываются нулевыми.

---

## Полный алгоритм (Флетчера-Ривса)

Полностью алгоритм описывается следующей последовательностью действий:

1.  В точке начального приближения $\bar{x}^0$ вычисляется $\bar{S}^0 = -\nabla f(\bar{x}^0)$.
2.  На $k$-м шаге с помощью [[Методы одномерного поиска|одномерного поиска]] в направлении $\bar{S}^k$ определяется минимум функции, то есть решается задача:
    $$\lambda^k = \arg \min_{\lambda} f(\bar{x}^k + \lambda \cdot \bar{S}^k)$$
    и находится очередное приближение $\bar{x}^{k+1} = \bar{x}^k + \lambda^k \cdot \bar{S}^k$.
3.  Вычисляется $f(\bar{x}^{k+1})$ и $\nabla f(\bar{x}^{k+1})$.
4.  Определяется направление $\bar{S}^{k+1} = -\nabla f(\bar{x}^{k+1}) + \omega_{k+1} \cdot \bar{S}^k$.
5.  Алгоритм заканчивается, если $||\bar{S}^k|| < \varepsilon$, где $\varepsilon$ – заданная величина.

> [!INFO] После $n+1$ итераций ($k=n$), если не произошел останов алгоритма, процедура циклически повторяется с заменой $\bar{x}^0$ на $\bar{x}^{n+1}$ и возвратом на первый пункт алгоритма.

> [!TIP] Если исходная функция является квадратичной, то $(n+1)$-е приближение даст точку экстремума данной функции. Описанный алгоритм с построением $\omega_k$ по формулам (6) соответствует методу сопряженных градиентов **Флетчера-Ривса**.

---

### Модификация Полака-Рибьера

В модификации Полака-Рибьера (Пшеничного) метод сопряженных градиентов отличается только вычислением $\omega_k$:
$$\omega_k = \frac{\nabla^T f(\bar{x}^k) \cdot [\nabla f(\bar{x}^k) - \nabla f(\bar{x}^{k-1})]}{||\nabla f(\bar{x}^{k-1})||^2}. \quad (7)$$
> [!INFO] В случае квадратичных функций обе модификации примерно эквивалентны, однако в модификации Полака-Рибьера алгоритм иногда оказывается несколько эффективнее.