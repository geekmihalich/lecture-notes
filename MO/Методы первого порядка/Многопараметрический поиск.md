
Милем и Кентреллом предложен метод поиска, основанный на использовании двух подбираемых параметров для минимизации $f(\bar{x})$ в каждом направлении поиска. В этом алгоритме последовательность действий определяется формулой:
$$\bar{x}^{k+1} = \bar{x}^k - \lambda_0^k \cdot \nabla f(\bar{x}^k) + \lambda_1^k \cdot \Delta \bar{x}^{k-1}, \quad (1)$$
где $\Delta \bar{x}^{k-1} = \bar{x}^{k-1} - \bar{x}^{k-2}$.

На каждом шаге решается задача минимизации по двум параметрам:
$$\min_{\lambda_0, \lambda_1} f(\bar{x}^k - \lambda_0^k \cdot \nabla f(\bar{x}^k) + \lambda_1^k \cdot \Delta \bar{x}^{k-1}).$$
После чего находится очередное приближение по формуле (1).

При этом можно показать, что:
$$\nabla^T f(\bar{x}^{k+1}) \cdot \nabla f(\bar{x}^k) = 0,$$
$$\nabla^T f(\bar{x}^{k+1}) \cdot \Delta \bar{x}^{k+1} = 0,$$
$$\nabla^T f(\bar{x}^{k+1}) \cdot \Delta \bar{x}^k = 0.$$

На первом шаге $\Delta \bar{x}^{-1} = 0$, а $\bar{x}^0$ должно быть задано.
На $k$-м шаге:
1.  Вычисляется $\bar{x}^k$, $\nabla f(\bar{x}^k)$ и $\Delta \bar{x}^{k-1} = \bar{x}^{k-1} - \bar{x}^{k-2}$.
2.  Пользуясь одним из эффективных методов, например, [[Методы второго порядка|методом Ньютона]], находятся с требуемой точностью $\lambda_0^k$ и $\lambda_1^k$.
3.  По соотношению (1) вычисляют $\bar{x}^{k+1}$ и переходят к пункту 1.
4.  Каждый $(n+1)$-й шаг начинается с $\Delta \bar{x}^{-1} = 0$.
5.  Процесс заканчивается, когда $\Delta f(\bar{x}) < \varepsilon$.

> [!INFO] На квадратичных функциях алгоритм по эффективности близок к [[Метод сопряженных градиентов|методу сопряженных градиентов]].

---

## Расширение на большее число параметров (Крэгг и Леви)

Крэгг и Леви распространили данный метод на случай большего числа параметров.
На каждом шаге очередное приближение находится как:
$$\bar{x}^{k+1} = \bar{x}^k - \lambda_0^k \cdot \nabla f(\bar{x}^k) + \sum_{i=1}^m \lambda_i^k \cdot \Delta \bar{x}_i^{k-1}$$
при $m \le n-1$, а, следовательно, на каждом шаге при минимизации $f(\bar{x})$ в заданном направлении решается задача вида:
$$\min_{\lambda_0, \lambda_1, \dots, \lambda_m} f\left(\bar{x}^k - \lambda_0^k \cdot \nabla f(\bar{x}^k) + \sum_{i=1}^m \lambda_i^k \cdot \Delta \bar{x}_i^{k-1}\right).$$

> [!NOTE] Достоинства и недостатки такого подхода очевидны.