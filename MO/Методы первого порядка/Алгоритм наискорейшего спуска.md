Градиент функции в любой точке показывает направление наибольшего локального увеличения $f(\bar{x})$. Поэтому при поиске минимума $f(\bar{x})$, следует двигаться в направлении противоположном направлению градиента $\nabla f(\bar{x})$ в данной точке, то есть в направлении наискорейшего спуска.

Итерационная формула процесса наискорейшего спуска имеет вид:
$$\bar{x}^{k+1} = \bar{x}^k - \lambda^k \cdot \nabla f(\bar{x}^k),$$
или (это просто переписано, чтобы показать направление $\bar{S}^k$)
$$\bar{x}^{k+1} = \bar{x}^k + \lambda^k \cdot \bar{S}^k, \quad \text{где } \bar{S}^k = -\frac{\nabla f(\bar{x}^k)}{||\nabla f(\bar{x}^k)||}.$$

> [!WARNING] Очевидно, что в зависимости от выбора параметра $\lambda$ траектории спуска будут существенно различаться. При большом значении $\lambda$ траектория спуска будет представлять собой колебательный процесс, а при слишком больших $\lambda$ процесс может расходиться. При выборе малых $\lambda$ траектория спуска будет плавной, но и процесс будет сходиться очень медленно.

Обычно $\lambda$ выбирают из условия:
$$\lambda^k = \arg \min_{\lambda} f(\bar{x}^k + \lambda \cdot \bar{S}^k),$$
решая одномерную задачу минимизации с использованием некоторого [[Методы одномерного поиска|одномерного метода]]. В этом случае получаем алгоритм наискорейшего спуска.

---

## Особенности алгоритма

> [!INFO] Если $\lambda$ определяется в результате одномерной минимизации, то градиент в точке очередного приближения будет ортогонален направлению предыдущего спуска $\nabla f(\bar{x}^{k+1}) \perp \bar{S}^k$. Это важно!

Вообще говоря, процедура наискорейшего спуска может закончиться в стационарной точке любого типа, в которой $\nabla f(\bar{x}) = \bar{0}$. Поэтому следует проверять, не завершился ли алгоритм в седловой точке.

> [!WARNING] Эффективность алгоритма зависит от вида минимизируемой функции. Алгоритм наискорейшего спуска сойдется за одну итерацию при любом начальном приближении для функции $f(\bar{x}) = x_1^2 + x_2^2$, но сходимость будет очень медленной, например, в случае функции вида $f(\bar{x}) = x_1^2 + 100x_2^2$.

В тех ситуациях, когда линии уровня минимизируемой функции представляет собой прямолинейный или, хуже того, криволинейный "овраг", эффективность алгоритма оказывается очень низкой.

> [!TIP] Очевидно, что хорошие результаты может давать предварительное масштабирование функций. Процесс наискорейшего спуска обычно быстро сходится вдали от точки экстремума и медленно в районе экстремума. Поэтому метод наискорейшего спуска нередко используют в комбинации с другими алгоритмами.