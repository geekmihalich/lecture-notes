
Внутри допустимой области строится гиперквадрат. В этом гиперквадрате случайным образом разбрасывается $m$ точек $\bar{x}_1, \dots, \bar{x}_m$, в которых вычисляются значения функции. Среди построенных точек выбираем наилучшую.

Таким образом, на 1-м этапе координаты случайных точек удовлетворяют неравенствам $a_i^1 \le x_i \le b_i^1, i=1,\dots,n$, и
$$\bar{x}^* = \arg \min_{j=1,\dots,m} \{f(\bar{x}_j)\} \quad \text{– точка с минимальным значением целевой функции.}$$
Опираясь на эту точку, строим новый гиперквадрат. Точка, в которой достигается минимум функции на $k$-м этапе, берется в качестве центра нового гиперквадрата на $(k+1)$-м этапе.

---

## Координаты вершин гиперквадрата

Координаты вершин гиперквадрата на $(k+1)$-м этапе определяются соотношениями:
$$a_i^{k+1} = \bar{x}_i^k - \frac{b_i^k - a_i^k}{2},$$
$$b_i^{k+1} = \bar{x}_i^k + \frac{b_i^k - a_i^k}{2},$$
где $\bar{x}_i^k$ – $i$-я координата наилучшей точки в гиперквадрате на $k$-м этапе.

В новом гиперквадрате выполняем ту же последовательность действий, случайным образом разбрасывая $m$ точек. В результате осуществляется направленное перемещение гиперквадрата в сторону уменьшения функции.

---

## Алгоритм с обучением (регулирование сторон)

В алгоритме с обучением стороны гиперквадрата могут регулироваться в соответствии с изменением по некоторому правилу параметра $\alpha$, определяющего стратегию изменения стороны гиперквадрата. В этом случае координаты вершин гиперквадрата на $(k+1)$-м этапе будут определяться соотношениями:
$$a_i^{k+1} = \bar{x}_i^k - \frac{b_i^k - a_i^k}{2\alpha},$$
$$b_i^{k+1} = \bar{x}_i^k + \frac{b_i^k - a_i^k}{2\alpha}.$$

> [!TIP] Хорошо выбранное правило регулирования стороны гиперквадрата приводит к достаточно эффективному алгоритму поиска.

В алгоритмах случайного поиска вместо направляющего гиперквадрата могут использоваться направляющие гиперсферы, направляющие гиперконусы.