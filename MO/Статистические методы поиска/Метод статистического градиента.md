Из исходного состояния $\bar{x}^k$ делается $m$ независимых проб $g \cdot \xi_1, \dots, g \cdot \xi_m$ в $m$ случайных направлениях, а затем вычисляются соответствующие значения минимизируемой функции в этих точках.

Для каждой пробы запоминаем приращения функции:
$$\Delta f_j^k = f(\bar{x}^k + g \cdot \xi_j) - f(\bar{x}^k).$$
После этого формируем векторную сумму:
$$\Delta \bar{f} = \sum_{j=1}^m \xi_j \cdot \Delta f_j^k.$$

> [!INFO] В пределе при $m \rightarrow \infty$ направление $\Delta \bar{f}$ совпадает с направлением градиента целевой функции.

При конечном $m$ вектор $\Delta \bar{f}$ представляет собой статистическую оценку направления градиента. В направлении $\Delta \bar{f}$ делается рабочий шаг и, в результате, очередное приближение $\bar{x}^{k+1}$ определяется соотношением:
$$\bar{x}^{k+1} = \bar{x}^k - \lambda \cdot \frac{\Delta \bar{f}}{||\Delta \bar{f}||}.$$

> [!TIP] При выборе оптимального значения $\lambda$, которое минимизирует функцию в заданном направлении, мы получаем **статистический вариант метода наискорейшего спуска**.

Существенное преимущество перед детерминированными алгоритмами заключается в возможности принятия решения о направлении рабочего шага при $m < n$.
При $m=n$ и неслучайных ортогональных рабочих шагах, направленных вдоль осей координат, алгоритм вырождается в градиентный метод.