
Методы переменной метрики называют также квазиньютоновскими или градиентными с большим шагом.

В этих методах в процессе поиска осуществляется аппроксимация матрицы вторых частных производных или обратной к ней. Причем для этого используются только первые производные.

Очередное приближение $\bar{x}^{k+1}$ в этих методах находится по формуле:
$$\bar{x}^{k+1} = \bar{x}^k + \lambda^k \cdot \bar{S}^k = \bar{x}^k - \lambda^k \cdot \eta(\bar{x}^k) \cdot \nabla f(\bar{x}^k), \quad (1)$$
где матрица $\eta(\bar{x}^k)$, которую иногда называют матрицей направлений, представляет собой аппроксимацию $H^{-1}(\bar{x}^k) = [\nabla^2 f(\bar{x}^k)]^{-1}$.

Для квадратичной целевой функции (или квадратичной аппроксимации целевой функции) имеем:
$$f(\bar{x}) \approx f(\bar{x}^k) + \nabla^T f(\bar{x}^k) \cdot (\bar{x} - \bar{x}^k) + \frac{1}{2}(\bar{x} - \bar{x}^k)^T \cdot \nabla^2 f(\bar{x}^k) \cdot (\bar{x} - \bar{x}^k),$$
где $\nabla^2 f(\bar{x}^k) = H(\bar{x}^k)$.

Если вместо $\bar{x}$ подставить в формулу $\bar{x}^{k+1}$ и продифференцировать, то получим:
$$\nabla f(\bar{x}^{k+1}) = \nabla f(\bar{x}^k) + H(\bar{x}^k) \cdot (\bar{x}^{k+1} - \bar{x}^k),$$
$$\nabla f(\bar{x}^{k+1}) - \nabla f(\bar{x}^k) = H(\bar{x}^k) \cdot (\bar{x}^{k+1} - \bar{x}^k).$$
Умножив на $H^{-1}(\bar{x}^k)$, получаем:
$$\bar{x}^{k+1} - \bar{x}^k = H^{-1}(\bar{x}^k) \cdot [\nabla f(\bar{x}^{k+1}) - \nabla f(\bar{x}^k)]. \quad (2)$$

> [!INFO] При этом если $f(\bar{x})$ квадратичная функция, то $H(\bar{x}^k) = H$ – постоянная матрица.
Уравнение (2) можно рассматривать как систему $n$ линейных уравнений с $n$ неизвестными параметрами, которые необходимо оценить для того, чтобы аппроксимировать $H^{-1}(\bar{x})$ или $H(\bar{x})$ при заданных значениях $f(\bar{x})$, $\nabla f(\bar{x})$ и $\Delta \bar{x}$ на более ранних этапах поиска.

> [!TIP] Для решения этих линейных уравнений могут быть использованы различные методы, каждый из которых приводит к различным методам переменной метрики.

---

## Обновление матрицы $\eta^k$

В большой группе методов матрица $H^{-1}(\bar{x}^{k+1})$ аппроксимируется с помощью информации, полученной на предыдущем $k$-м шаге:
$$\eta^{k+1} \approx \omega \cdot \eta^k = \omega \cdot (\eta^k + \Delta \eta^k), \quad (3)$$
где $\eta^k$ – матрица, аппроксимирующая $H^{-1}(\bar{x}^k)$ на предыдущем шаге. Вообще $\eta^k = \eta(\bar{x}^k)$. В (3) $\Delta \eta^k$ представляет собой определяемую матрицу, а $\omega$ - масштабный (постоянный) множитель, в большинстве случаев равный единице.

> [!INFO] Выбор $\Delta \eta^k$ по существу и определяет соответствующий метод переменной метрики.

Для обеспечения сходимости метода матрица $\omega \cdot \eta^{k+1}$ должна быть положительно определенной и удовлетворять уравнению (2) в том случае, когда она заменяет $H^{-1}$.
На $(k+1)$-м шаге мы знаем $\bar{x}^k$, $\nabla f(\bar{x}^k)$, $\nabla f(\bar{x}^{k+1})$ и $\eta^k$ и нам требуется вычислить $\eta^{k+1}$ так, чтобы удовлетворялось соотношение (2).
Из выражения (2) с учетом (3):
$$\Delta \bar{x}^k = \omega \cdot \eta^k \cdot [\nabla f(\bar{x}^{k+1}) - \nabla f(\bar{x}^k)] = \omega \cdot \eta^k \cdot \Delta \bar{g}^k.$$
Отсюда:
$$\Delta \eta^k \cdot \Delta \bar{g}^k = \frac{1}{\omega} \Delta \bar{x}^k. \quad (4)$$
Так как $\eta^{k+1} = \eta^k + \Delta \eta^k$, то на основании (4) уравнение:
$$\Delta \eta^k \cdot \Delta \bar{g}^k = \frac{1}{\omega} \Delta \bar{x}^k - \eta^k \cdot \Delta \bar{g}^k \quad (5)$$
следует разрешить относительно $\Delta \eta^k$.

Прямой подстановкой результата можно показать, что уравнение (5) имеет следующее решение:
$$\Delta \eta^k = \frac{1}{\omega} \frac{\Delta \bar{x}^k \Delta \bar{y}^T - \eta^k \Delta \bar{g}^k \Delta \bar{z}^T}{\Delta \bar{y}^T \cdot \Delta \bar{g}^k}, \quad (6)$$
где $\bar{z}$ и $\bar{y}$ – произвольные векторы размерности $n$.

> [!INFO] Если в этих алгоритмах шаги $\Delta \bar{x}^k$ строятся последовательно в результате минимизации функции $f(\bar{x})$ в направлении $\bar{S}^k$, то все методы, с помощью которых вычисляют симметрическую матрицу $\eta^{k+1}$, удовлетворяющую соотношению (4), дают направления, являющиеся взаимно сопряженными (в случае квадратичной целевой функции).

---

### Основные методы переменной метрики:

*   [[Метод Бройдена]]
*   [[Метод Дэвидона-Флетчера-Пауэлла]]
*   [[Методы Гринштадта и Гольдфарба]]
*   [[Алгоритм Флетчера]]
*   [[Алгоритмы с аппроксимацией матрицы Гессе]]
*   [[Алгоритм Гольштайна и Прайса]]