Можно строить аналогичные алгоритмы, аппроксимируя в процессе поиска не матрицу $H^{-1}(\bar{x}^k)$, а матрицу $H(\bar{x}^k)$. А затем строить обратную к ней.

Таким образом, на каждой итерации находится $\Gamma^{k+1} \approx H(\bar{x}^{k+1}) = \Gamma^k + \Delta \Gamma^k$, где $\Gamma^k$ – оценка $H(\bar{x}^k)$, а матрица $\Delta \Gamma^k$ – симметрическая матрица ранга 1, такая, что $\Gamma^{k+1} \Delta \bar{x}^k = \Delta \bar{g}^k$.

При этом:
$$\Delta \Gamma^k = \frac{[\Delta \bar{g}^k - \Gamma^k \cdot \Delta \bar{x}^k] \cdot [\Delta \bar{g}^k - \Gamma^k \cdot \Delta \bar{x}^k]^T}{[\Delta \bar{g}^k - \Gamma^k \cdot \Delta \bar{x}^k]^T \cdot \Delta \bar{x}^k}. \quad (15)$$

> [!WARNING] Поскольку в процессе поиска матрица $[\Gamma^k]^{-1}$ может не быть положительно определенной, следует в процессе поиска использовать ограничительные условия, обеспечивающие положительную определенность такой матрицы.

В качестве начального приближения можно выбирать $\Gamma^0 = [\eta^0]^{-1}$.