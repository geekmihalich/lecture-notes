
> [!INFO] В методах второго порядка при поиске минимума используют информацию о функции и ее **производных до второго порядка включительно**. К этой группе относят [[Метод Ньютона и его модификации|метод Ньютона]] и его модификации.

В основе метода лежит квадратичная аппроксимация $f(\bar{x})$, которую можно получить, отбрасывая в рядах Тейлора члены третьего и более высокого порядков:
$$f(\bar{x}) \approx f(\bar{x}^k) + \nabla^T f(\bar{x}^k) \cdot (\bar{x} - \bar{x}^k) + \frac{1}{2}(\bar{x} - \bar{x}^k)^T \cdot \nabla^2 f(\bar{x}^k) \cdot (\bar{x} - \bar{x}^k), \quad (1)$$
где $\nabla^2 f(\bar{x}^k) = H(\bar{x}^k)$ – **матрица Гессе**, представляющая собой квадратную матрицу вторых частных производных $f(\bar{x})$ в точке $\bar{x}^k$.

---

## Метод Ньютона

Направление поиска $\bar{S}^k$ в методе Ньютона определяется следующим образом.
Если заменить в выражении (1) $\bar{x}$ на $\bar{x}^{k+1}$ и обозначить $\Delta \bar{x}^k = \bar{x}^{k+1} - \bar{x}^k$, то получим:
$$f(\bar{x}^{k+1}) \approx f(\bar{x}^k) + \nabla^T f(\bar{x}^k) \cdot \Delta \bar{x}^k + \frac{1}{2}(\Delta \bar{x}^k)^T \cdot \nabla^2 f(\bar{x}^k) \cdot \Delta \bar{x}^k. \quad (2)$$
Минимум функции $f(\bar{x})$ в направлении $\Delta \bar{x}^k$ определяется дифференцированием $f(\bar{x})$ по каждой из компонент $\Delta \bar{x}$ и приравниванием к нулю полученных выражений:
$$\nabla f(\bar{x}^k) + \nabla^2 f(\bar{x}^k) \cdot \Delta \bar{x}^k = \bar{0}. \quad (3)$$
Это приводит к:
$$\Delta \bar{x}^k = -[\nabla^2 f(\bar{x}^k)]^{-1} \cdot \nabla f(\bar{x}^k), \quad (4)$$
$$\bar{x}^{k+1} = \bar{x}^k - [\nabla^2 f(\bar{x}^k)]^{-1} \cdot \nabla f(\bar{x}^k). \quad (5)$$

> [!INFO] В данном случае и величина шага и направление поиска определены точно. Если $f(\bar{x})$ - квадратичная функция (выпуклая вниз), то для достижения минимума достаточно **одного шага**.

> [!WARNING] Но в общем случае нелинейной функции $f(\bar{x})$ за один шаг минимум не достигается.

Поэтому итерационную формулу (5) обычно приводят к виду:
$$\bar{x}^{k+1} = \bar{x}^k - \lambda^k \cdot [\nabla^2 f(\bar{x}^k)]^{-1} \cdot \nabla f(\bar{x}^k), \quad (6)$$
где $\lambda^k$ - параметр длины шага, или к виду:
$$\bar{x}^{k+1} = \bar{x}^k - \lambda^k \cdot H^{-1}(\bar{x}^k) \cdot \nabla f(\bar{x}^k). \quad (7)$$
Направление поиска определяется вектором:
$$\bar{S}^k = -H^{-1}(\bar{x}^k) \cdot \nabla f(\bar{x}^k).$$
Итерационный процесс (6) или (7) продолжается до тех пор, пока не будет выполнен некоторый критерий останова.

> [!INFO] Условием, гарантирующим сходимость метода Ньютона в предположении, что функция $f(\bar{x})$ дважды дифференцируема, заключается в том, что матрица $H^{-1}(\bar{x}^k)$ должна быть положительно определенной.

---

## Градиентные методы vs. Метод Ньютона

*   > [!INFO] Градиентные методы, в частности [[Алгоритм наискорейшего спуска|метод наискорейшего спуска]], обладают **линейной скоростью сходимости**.
*   > [!INFO] Метод Ньютона обладает **квадратичной скоростью сходимости**.

> [!WARNING] Применение метода Ньютона оказывается очень эффективным при условии, что выполняются необходимые и достаточные условия его сходимости. Однако само исследование необходимых и достаточных условий сходимости метода в случае конкретной $f(\bar{x})$ может быть достаточно сложной задачей.

---

## Модификации метода Ньютона

> [!WARNING] Иногда определенную сложность вызывает вычисление на каждом шаге матрицы $H^{-1}(\bar{x}^k)$.

Тогда вместо метода Ньютона используют его модификацию. Суть модифицированного метода Ньютона заключается в том, что при достаточно хорошем начальном приближении вычисляется матрица $[\nabla^2 f(\bar{x}^0)]^{-1}$ и в дальнейшем на всех итерациях вместо $[\nabla^2 f(\bar{x}^k)]^{-1}$ используется $[\nabla^2 f(\bar{x}^0)]^{-1}$.
Очередные приближения определяются соотношением:
$$\bar{x}^{k+1} = \bar{x}^k - \lambda^k \cdot [\nabla^2 f(\bar{x}^0)]^{-1} \cdot \nabla f(\bar{x}^k). \quad (8)$$
Естественно, что число итераций, необходимое для достижения минимума, обычно возрастает, но в целом процесс может оказаться экономичнее.

> [!INFO] Метод быстро сходится в выпуклой окрестности точки минимума, где минимизируемая функция хорошо аппроксимируется квадратичной функцией.

### Методы переменной метрики

*   [[Методы переменной метрики]]